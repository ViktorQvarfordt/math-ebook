<h1>Abstract algebra</h1>

<h2>Operations and algebraic structures</h2>

<definition title="Binary operation" tag="definition-binary-operation">
  <p>
    A <em>binary operation</em> on a [set] $A$ is a [function] $*: A \times A \to B$ where $A \subseteq B$.
    We use the <em>infix notation</em> $a*b$ to denote $*(a,b)$.
  </p>
</definition>

<definition title="Associative" tag="definition-associative definition-associativity">
  <p>
    A [binary operation] $*$ on a [set] $A$ is such that
    $$\forall a, b, c \in A \, (a*(b*c) = (a*b)*c).$$
  </p>
</definition>

<definition title="Commutative">
  <p>
    A [binary operation] $*$ on a [set] $A$ is such that
    $$\forall a, b \in A \, (a*b = b*a).$$
  </p>
</definition>

<example>
  <p>
    The operations addition and multiplication on $\mathbb{R}$ are associative and commutative. However, subtraction and division are neither.
  </p>
  <raw lang="julia">
⋆ = (a, b) -> a - b
a, b, c = 1, 2, 3
println("test of associativity: ", (a ⋆ (b ⋆ c), (a ⋆ b) ⋆ c))
println("test of commutativity: ", (a ⋆ b, b ⋆ a))
  </raw>
  Outputs:
  <raw>
test of associativity: (2, -4)
test of commutativity: (-1, 1)
  </raw>
</example>


<definition title="Identity element">
  <p>
    An element $e \in A$ is an <em>identity element</em> if it satisfies both:
  </p>
  <ul class="inline">
    <li>$\forall a \in A \, (e*a = a)$ (<em>left identity</em>)</li>
    <li>$\forall a \in A \, (a*e = a)$ (<em>right identity</em>)</li>
  </ul>
</definition>

<proposition title="Uniqueness of identity">
  <p>
    If a binary operation has an identity element, then it is unique.
  </p>
  <proof>
    <p>
      Let $(A, *)$ be a binary operation. Assume that both $e$ and $f$ are identity elements of $*$. Then,
    </p>
    <ol class="inline">
      <li>$e * f = e$, because $e$ is a left identity.</li>
      <li>$e * f = f$, because $f$ is a right identity.</li>
      <li>Since the left hand sides of the above equations are equal, also the right hand sides must be equal, thus $e = f$.</li>
    </ol>
  </div>
</proposition>

<definition title="Closure">
  <p>
    Let $* : A \times A \to B$ be a binary operation.
  </p>
  <ul class="inline">
    <li>We say that $*$ is <em>closed</em> if $*(A) \subseteq A$.</li>
    <li>Let $A' \subseteq A$. We say that $A'$ has <em>closure</em> under $*$, or that $*$ is <em>closed</em> on $A'$, if $*(A') \subseteq A'$.</li>
  </ul>
  <intuition>
    <p>
      Let $*$ be a binary operation on $\mathbb{Z}$ given by $a*b = a - b$. This operation is not closed on $\mathbb{Z}^+$ because there exists $a, b \in \mathbb{Z}^+$ such that $a - b \not\in \mathbb{Z}^+$ (for example $a = 0, b = 1$). On the other hand the binary operation $(\mathbb{Z}, +)$ can be restricted to a binary operation on $\mathbb{Z}^+$.
    </p>
  </intuition>
</definition>

<definition title="Algebraic structure">
  <p>
    An <strong>algebraic structure</strong> is a tuple $(A_1, \ldots, A_n, *_1, \ldots, *_m)$ where $A_1, \ldots, A_n$ are [sets](set) and $*_1, \ldots, *_m$ are [binary operations](binary operation).
  </p>
  <remark title="Standard abuse of notation">
    <p>
      If $S = (A_1, \ldots, A_n, *_1, \ldots, *_m)$ is an algebraic structure, then we write $x \in S$ to denote $x \in A_1$.
    </p>
  </remark>
</definition>

<definition title="Magma">
  <p>
    A <em>magma</em> is a a tuple $(A, *)$ where $A$ is a set and $*$ is a closed binary operation on $A$.
  </p>
</definition>

<definition title="Monoid">
  <p>
    A <em>monoid</em> is a magma $(A, *)$ such that $*$ is associative and has identity.
  </p>
</definition>

<definition title="Inverse element">
  <p>
    Let $(A, *)$ be a magma with identity element $e$. An element $a^{-1} \in A$ is an <em>inverse</em> of $a \in A$ if it satisfies both:
  </p>
  <ul class="inline">
    <li>$a^{-1} * a = e$  (<em>left inverse</em>)</li>
    <li>$a * a^{-1} = e$ (<em>right inverse</em>)</li>
  </ul>
</definition>

<proposition title="Uniqueness of inverse">
  <p>
    Let $(A, *)$ be a monoid, let $a \in A$. If $a$ has an inverse $a^{-1}$, then it is unique.
  </p>
  <proof>
    <p>
      Let $(A, *)$ be a monoid. Let $a \in A$. Assume that both $a^{-1}$ and $b^{-1}$ are inverse elements of $a$. Then,
    </p>
    <ol class="inline">
      <li>$e = a^{-1} * a$.</li>
      <li>$e = b^{-1} * a$.</li>
      <li>The above together with uniqueness of identity implies $a^{-1} * a = b^{-1} * a$.</li>
      <li>Operating with $a^{-1}$ from right gives $(a^{-1} * a) * a^{-1} = (b^{-1} * a) * a^{-1}$.</li>
      <li>By associativity $(a^{-1} * a) * a^{-1} = (b^{-1} * a) * a^{-1} \implies a^{-1} * (a * a^{-1}) = b^{-1} * (a * a^{-1})$.</li>
      <li>By propery of inverse $a^{-1} * e = b^{-1} * e \implies a^{-1} = b^{-1}$.</li>
    </ol>
  </proof>
  <remark>
    <p>
      The theorem does not hold if $*$ is not associative [<a href="https://proofwiki.org/wiki/Inverse_not_always_Unique_for_Non-Associative_Operation">proof</a>].
    </p>
  </remark>
</proposition>

<definition title="Group">
  <p>
    A <strong>group</strong> is a [monoid] $(G, *)$ such that each element of $G$ has an [inverse element] under $*$, denoted $-x$ (<em>additive notation</em>) or $x^{-1}$ (<em>multiplicative notation</em>).
  <p>
    An alternative notation for a group $(G, *)$ is $(G, m, i)$ where $m$ is the <em>multiplication function</em> and $i$ is the <em>inverse element function</em>, given by:
  </p>
  <ul class="inline">
    <li>$m : G \times G \to G; \; (x, y) \mapsto x * y$</li>
    <li>$i : G \to G; \; x \mapsto x^{-1}$</li>
  </ul>
</definition>

<definition title="Anticommutative">
  <p>
    Let $(G, *)$ be a [group], then $*$ is <strong>anticommutative</strong> if
    $$\forall x, y \in G \, \big( x * y = - (y * x) \big)$$
  </p>
</definition>

<definition title="Compatibility of operations" tag="definition-compatibility-of-operations definition-compatible-operations">
  <p>
    Let $*$ and $\star$ be two binary operations on a set $A$, we say that $*$ and $\star$ are <em>compatible</em> if
  </p>
  <ul class="inline">
    <li>$\forall a, b, c \in A \, \big(a * (b \star c) = (a * b) \star c\big)$</li>
  </ul>
</definition>

<definition title="Monoid action">
  <p>
    Let $(A, *)$ be a <a href="#definition-monoid">monoid</a> with identity $e$ and let $X$ be a set. A (left) <strong>monoid action</strong> of $(A, *)$ on $X$ is a function $\star : A \times X \to X$ such that
  </p>
  <ul class="inline">
    <li>$\forall x \in X \, (e \star x = x)$ (compatibility of identity).</li>
    <li>$*$ and $\star$ are [compatible](compatible operations).</li>
  </ul>
  <p>Let $G$ be a group, then a group action of $G$ on $X$ is a monoid action of $G$ on $X$. (This makes sense because a group is a monoid).</p>
</definition>

<definition title="Distributivity">
  <p>
    Let $*$ and $\star$ be two binary operations on a set $A$, we say that $*$ is <strong>distributive</strong> over $\star$ if both
  </p>
  <ul class="inline">
    <li>$\forall a, b, c \in A \, \big(a * (b \star c) = (a * b) \star (a * c)\big)$ (left distributive)</li>
    <li>$\forall a, b, c \in A \, \big((b \star c) * a = (b * a) \star (c * a)\big)$ (right distributive)</li>
  </ul>
</definition>

<definition title="Abelian">
  <p>
    An algebraic structure $(A, *)$ is <strong>abelian</strong> or <strong>commutative</strong> if $*$ is [commutative].
  </p>
</definition>

<definition title="Ring">
  <p>
    A <strong>ring</strong> $R$ is a 3-tuple $(A, +, \cdot)$ where $A$ is a [set] and $+, \cdot$ are <a href="#definition-binary-operation">binary operations</a> on $A$ such that:
  </p>
  <ul class="inline">
    <li>$(A, +)$ is an [abelian] [group] with [identity element] $0_R$.</li>
    <li>$(A, \cdot)$ is a [monoid] with identity element $1_R$.</li>
    <li>Multiplication is <a href="#definition-distributivity">distributive</a> over addition.</li>
  </ul>
</definition>

<intuition title="Overview of algebraic structures">
  <p>
    <a href="https://en.wikipedia.org/wiki/Template:Group-like_structures">Here</a> is a table showing an overview of algebraic structures. Below we show code for implementing the group axioms for finite groups.
  </p>
  <raw lang="julia">
# In the following we let:
# - s be short for set, acting as the domain
# - op or * be a binary operator on s
# - e be an identity for *
# - i be the inverse for *

function hasclosure(s, *)
    for x ∈ s, y ∈ s
        if x * y ∉ s
            println("closure does not hold because $(x) * $(y) = $(x * y) is not in the domain")
            return false
        end
    end
    return true
end

ismagma = hasclosure

function isinversefunction(s, *, i, e)
    for x ∈ s
        if x * i(x) != e || i(x) * x != e
            println("inverse does not hold because $(x) * ($(x))^{-1} ≠ e = $(e)")
            return false
        end
    end
    return true
end

function isassociative(s, *)
    for a ∈ s, b ∈ s, c ∈ s
        if (a * b) * c != a * (b * c)
            println("associativity does not hold because ($(a) * $(b)) * $(c) ≠ $(a) * ($(b) * $(c)))")
            return false
        end
    end
    return true
end

function isidentity(s, *, e)
    for a ∈ s
        if e * a ≠ a
            println("identity does not hold because $(e) * $(a) ≠ $(a)")
            return false
        elseif a * e ≠ a
            println("identity does not hold because $(a) * $(e) ≠ $(a)")
            return false
        end
    end
    return true
end

ismonoid(s, *, e) = ismagma(s, *) && isidentity(s, *, e) && isassociative(s, *)

isgroup(s, *, i, e) = ismonoid(s, *, e) && isinversefunction(s, *, i, e)

isring(s, op1, i1, e1, op2, e2) = isgroup(s, op1, i1, e1) && ismonoid(s, op2, e2)


# Code for generating groups

function get_add_group(n)
    s = 0:(n-1)
    op = (a, b) -> mod(a + b, n)
    e = 0
    i = a -> mod(-a, n)
    return (s, op, i, e)
end

function get_mult_group(n)
    get_coprimes(n) = filter(a -> gcd(a, n) == 1, 0:(n-1))

    function multinv(s, *, a, e)
        # This is a brute force method, it can be done faster with the Extended Euclidean algorithm
        for b ∈ s
            if a * b == e # The operation * is abelian, so left inverse implies inverse
                return b
            end
        end
        println("$(a) has no inverse")
        return nothing
    end

    s = get_coprimes(n)
    println("the domain is $(s)")
    op = (a, b) -> mod(a * b, n)
    e = 1
    i = a -> multinv(s, op, a, e)

    return (s, op, i, e)
end

println(isgroup(get_add_group(5)...))
println(isgroup(get_mult_group(5)...))
  </raw>
</intuition>

<definition title="Module">
  <p>
    A <em>module</em> $M$ is a [tuple] $(A, R, +, \cdot)$ where $A$ is a [set], $R$ is a [ring], $+$ (<em>module addition</em>) is a [function] $A\times A \to A$ and $\cdot$ (<em>scalar multiplication</em>) is [function] $R \times A \to A$ such that:
  </p>
  <ol class="inline-top">
    <li>$\forall x \in X \, (1_R \cdot x = x)$ (compatibility of identity).</li>
    <li>Scalar multiplication is <a href="#definition-compatibility-of-operations">compatible</a> with ring multiplication.</li>
    <li>$(A, +)$ is an [abelian] [group].</li>
    <li>$\forall r \in R, x, y \in X \, \big(r \cdot (x + y) = r \cdot x + r \cdot y \big)$.</li>
    <li>$\forall r, s \in R, x \in X \, \big((r +_R s) \cdot x = r \cdot x + s \cdot x \big)$.</li>
  </ol>
  <p>
    We say that $(A, R, +, \cdot)$ is a module <em>over</em> the ring $R$. A module should be thought of as a ring action on an abelian group. Indeed, the first two requirements are inherited from <a href="#definition-monoid-action">monoid action</a> for the monoid $(R, \cdot_R)$, the third requirement confirms that the ring is acting on an abelian group and the last two requirements describe the compatibility of the operations of the ring $(R, +_R, \cdot_R)$ and the abelian group $(A, +)$.
  </p>
</definition>

<definition title="Field">
  <p>
    A <em>field</em> $F$ is a [ring] $R = (A, +, \cdot)$ such that $(A\setminus\{0\}, \cdot)$ is an [abelian] [group] with $1_R$ as [identity element].
  </p>
</definition>

<definition title="Vector space">
  <p>
    A <em>vector space</em> is a <a href="#definition-module">module</a> over a <a href="#definition-field">field</a>.
  </p>
</definition>

<definition title="Homomorphism">
  <p>
    A <em>homomorphism</em> between two algebraic structures is a map between the algebraic structures that preserves the structure.
  </p>
</definition>

<definition title="Isomorphism">
  <p>
    An <em>isomorphism</em> is a [homomorphism] whose <a href="#definition-inverse-function">inverse</a> is a [homomorphism].
  </p>
</definition>

<definition title="Module homomorphism" tag="definition-module-homomorphism definition-linear-function">
  <p>
    Let $(M,R,+_M,\cdot_M)$ and $(N,R,+_N,\cdot_N)$ be <a href="#definition-module">modules</a> and let $f : M \to N$, then $f$ is a <em>module homomorphism</em> if $\forall x, y \in M, r \in R$:
  </p>
  <ul class="inline">
    <li>$f(x +_M y) = f(x) +_N f(y)$</li>
    <li>$f(r \cdot_M x) = r \cdot_N f(x)$</li>
  </ul>
  <p>
    Module homomorphisms are often (in particular in linear algebra) called <em>linear functions</em>.
  </p>
</definition>

<definition title="Multilinear" tag="definition-multilinear definition-bilinear definition-multilinear-form definition-bilinear-form">
  <p>
    Let $(M_k,R,+_{M_k},\cdot_{M_k}), 1 \le k \le n$ and $(N,R,+_N,\cdot_N)$ be [modules](module) and let $f : \prod_{k=1}^n M_k \to N$, then $f$ is a <em>multilinear function</em> if $f$ restricted to $M_k$ is a [module homomorphism] for $1 \le k \le n$.
  </p>
  <p>
    If $f$ is multilinear with $n = 2$ we say that $f$ is <strong>bilinear</strong>.
  </p>
  <p>
    If $f$ is multilinear with $M_j = M_k$ for all $i \le j, k \le n$ we say that $f$ is a <strong>multilinear form</strong>.
  </p>
</definition>

<definition title="Alternating">
  <p>
    Let $f : M^n \to N$ be a [multilinear form], then $f$ is <strong>alternating</strong> if it satisfies
    $$
      \forall x_1, \ldots, x_n \in M,\, \forall j \ne k \;
      \allowbreak
      \big( f(x_1, \ldots, x_j, \ldots, x_k, \ldots, x_n) =
      - f(x_1, \ldots, x_k, \ldots, x_j, \ldots, x_n) \big).
    $$
    It follows that if $x_j = x_k$ for $j \ne k$ then $f(x_1, \ldots, x_j, \ldots, x_k, \ldots, x_n) = 0$.
  </p>
</definition>

<definition title="Free module">
  <p>
    A free module is ... Some nice examples highlighting the difference from vector spaces <a href="https://kurser.math.su.se/pluginfile.php/14420/mod_resource/content/1/yishao-zhou-DifferenceModulesVS.pdf">here</a>.
  </p>
</definition>



<h2>Linear algebra</h2>

<p>Linear algebra can be thought of as a subarea of abstract algebra, dealing with vector spaces, primarily deals the real vector space $\mathbb{R}^n$.</p>

<example title="The real vector space $ℝ^n$" tag="example-real-vector-space">
  <p>
    A self-contained description of the prototypical vector space, the <em>real vector space</em> $ℝ^n$.
  </p>
  <p>
    Consider the $n$-fold cartesian product $ℝ^n$ of the real numbers $ℝ$. The points in $ℝ^n$ are called <em>vectors</em>. Vector addition is given by component-wise addition; if $x, y \in ℝ^n$ then
    $$
    \begin{aligned}
      x + y &= (x_1, x_2, \ldots, x_n) + (y_1, y_2, \ldots, y_n) \\
      &= (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n),
    \end{aligned}
    $$
    and scalar multiplication with $k \in ℝ$ is given by
    $$
    \begin{aligned}
      kx &= k(x_1, x_2, \ldots, x_n) \\
      &= (kx_1, kx_2, \ldots, kx_n).
    \end{aligned}
    $$
  </p>
  <p>
    As an exercise, check that the this indeed is a vector space according to the abstract definition.
  </p>

  <p>
    <strong>Implementation of $ℝ^3$ in Julia:</strong>
  </p>

  <raw lang="julia">
import Base.+
import Base.*

Scalar = Number

# Define a type to represent 3-dimensional vectors
struct Vec3d
    x::Scalar
    y::Scalar
    z::Scalar
end

# Define vector addition
function +(u::Vec3d, v::Vec3d)::Vec3d
    return Vec3d(u.x + v.x, u.y + v.y, u.z + v.z)
end

# Define scalar multiplication
function *(k::Scalar, v::Vec3d)
    return Vec3d(k * v.x, k * v.y, k * v.z)
end

u = Vec3d(1, 2, 3)
v = Vec3d(1, 1, 1)

k = 5

println("test of vector addition: ", u + v)
println("test of scalar multiplication: ", k * u)
 </raw>

  <raw>
test of vector addition: Vec3d(2, 3, 4)
test of scalar multiplication: Vec3d(5, 10, 15)
  </raw>
</example>

<definition title="Scalar product">
  <p>
    Let $u, v \in \mathbb{R}^n$, the <em>scalar product</em> of $u$ and $v$ is given by
    $$ u \cdot v = \sum_{k=1}^n u_k v_k. $$
  </p>
</definition>

<observation title="Inner product and geometry">
  <p>
    Let $u, v \in \mathbb{R}^n$ and let $\theta$ be the angle $u$ and $v$, then
    $$ u \cdot v = |u| |v| \cos \theta. $$
  </p>
  <proof>
    <p>
      Using the [law of cosines] and then expanding resulting expression proves the theorem.
      $$\begin{aligned}
        |u - v|^2 &= |u|^2 + |v|^2 + 2|u||v|\cos \theta \\
        \iff |u||v|\cos \theta &= \frac{1}{2}\left(|u|^2 + |v|^2 - |u - v|^2 \right) \\
        &= \frac{1}{2}\left( \sum u_k^2 + v_k^2 - (u_k - v_k)^2 \right) \\
        &= \frac{1}{2}\left( \sum u_k^2 + v_k^2 - u_k^2 - v_k^2 + 2u_kv_k \right) \\
        &= \sum u_kv_k \\
        &= u \cdot v
      \end{aligned}$$
    </p>
  </proof>
  <p>
    Note that this result depends on the [law of cosines], i.e. requiring that the space has a corresponding Euclidean geometry in which notions of distances and angles are defined. However, in the general setting of a vector space with an inner product, this result can be used to <em>define</em> the notion of angles.
  </p>
  <p>
    In this sense an inner product defines a geometry of a vector space, endowing it with the notion of distances and angles by defining
    $$\begin{aligned}
      |u| &= \sqrt{\langle u, u \rangle} \\
      \angle(u, v) &= \arccos\left( \frac{\langle u, v \rangle}{|u| |v|} \right)
    \end{aligned}$$
    for vectors $u$ and $v$ where $|u|$ denotes the length (distance) of $u$ and $\angle(u, v)$ denotes the angle between $u$ and $v$.
  </p>
</observation>

<definition title="Inner product">
  <p>
    The inner product generalizes the scalar product.
  </p>
</definition>

<p>TODO: https://www.math.usm.edu/perry/old_classes/mat681sp14/norm_and_metric.pdf</p>

<definition title="Linear combination">
  <p>
    A linear combination of the vectors $v_1, \ldots, v_n \in V$ is an expression on the form
    $$\sum_{k=1}^n a_k v_k$$
    where $a_k \in \text{field}(V)$.
  </p>
</definition>

<definition title="Linear dependence">
  <p>
    The vectors $v_1, \ldots, v_n$ are said to be linearly dependent if
    $$\sum_{k=1}^n a_k v_k = 0$$
    for scalars $a_k$, not all zero.
  </p>
  <intuition>
    Equivalent to that one of the vectors is a linear combination of the others.
  </intuition>
</definition>

<definition title="Span">
  <p>
    Let $v_1, \ldots, v_n$ be vectors of a vector space $V$. The span of the vectors is the set of all linear combinations of the vectors,
    $$\text{span}\{v_1, \ldots, v_n\} = \left\{\sum_{k=1}^n a_k v_k : a_k \in \text{field}(V)\right\}.$$
  </p>
  <p>
    If $\text{span}\{v_1, \ldots, v_n\} = V$ we say that the vectors span $V$ or that the vectors form a spanning set.
  </p>
</definition>

<definition title="Basis">
  <p>
    A set of vectors $\{v_1, \ldots, v_n\}$ of a vector space $V$ is a basis for $V$ if the vectors are linearly independent and $\text{span}\{v_1, \ldots, v_n\} =  V$.
  </p>
</definition>


<definition title="Linear map">
  <p>
    A linear map is a module homomorphism between two vector spaces.
  </p>
</definition>

<h3>Change of basis</h3>

<p>
  Let $\alpha = \{\alpha_j\}_{j=1}^n$ and $\beta = \{\beta_j\}_{j=1}^n$ be two bases for $V$ with coordinate maps
  $$
  \begin{aligned}
    \varphi_1 &: \mathbb{R}^n \to V; \; e_j \mapsto \alpha_j \\
    \varphi_2 &: \mathbb{R}^n \to V; \; e_j \mapsto \beta_j
  \end{aligned}
  $$
  where $\{e_j\}_{j=1}^n$ is the standard basis for $\mathbb{R}^n$.
</p>


<h4>Change of basis for vectors</h4>

<p>
  A vector $\xi \in V$ has coordinates $\varphi_1^{-1}(\xi)$ in basis $\alpha$ and $\varphi_2^{-1}(\xi)$ in basis $\beta$.
  Thus we have
  $$
  \begin{aligned}
    \varphi_2^{-1}(\xi) = (\varphi_2^{-1} \varphi_1) \varphi_1^{-1}(\xi).
  \end{aligned}
  $$
  From this, define the <strong>change of basis matrix</strong>
  $$
  \begin{aligned}
    P^{-1} = \varphi_2^{-1} \varphi_1,
  \end{aligned}
  $$
  i.e. the matrix that takes a vector in $\alpha$-coordinates and gives the $\beta$-coordinate representation.
  The matrix $P = \varphi_1^{-1} \varphi_2$ is easily computed by the following observation,
  $$
  \begin{aligned}
    P(e_j) = \varphi_1^{-1} \varphi_2 (e_j) = \varphi_1^{-1} (\beta_j),
  \end{aligned}
  $$
  i.e. the $j$:th column of $P$ is the coordinates for the new basis vector $\beta_j$ in the old basis $\alpha$.
</p>


<h4>Change of basis for matrices</h4>

<p>
  Let $T : V \to V$ be a linear transformation from $V$ to itself (i.e. $T$ is an endomorphism).
  The matrix representation for $T$ in the two coordinates are
  $$
  \begin{aligned}
    &T_\alpha = \varphi_1^{-1} T \varphi_1, \\
    &T_\beta  = \varphi_2^{-1} T \varphi_2.
  \end{aligned}
  $$
  With $P^{-1}$ as above, we have
  $$
  \begin{aligned}
    T_\beta = P^{-1} T_\alpha P.
  \end{aligned}
  $$
</p>


<h4>Remark</h4>

<p>
  The similarity transform $SAS^{-1}$ can be viewed as follows: You give it a coordinate vector in terms of the basis $\beta$ made up of the columns of $S^{-1} = P$. Then $S^{-1}$ translates this into the standard basis; then you apply $A$ as usual; then you apply $S$ and translate it back into the basis $\beta$. So you can view $SAS^{-1}$ as performing $A$, but in terms of the basis $\beta$.
</p>


<h4>Dirac notation</h4>

<p>
  Let $\{e_j\}_j$ and $\{f_j\}_j$ be two orthonormal bases. We thus have two resolutions of the identity,
  $$
    I =
    \sum_j \vert e_j \rangle\langle e_j \vert =
    \sum_j \vert f_j \rangle\langle f_j \vert.
  $$
</p>

<h5>Vectors</h5>

<p>
  A vector $\vert\psi\rangle$ can be expanded in either basis
  $$
    \vert\psi\rangle =
    \sum_j \vert e_j \rangle\langle e_j \vert\psi\rangle =
    \sum_j \vert f_j \rangle\langle f_j \vert\psi\rangle.
  $$
  The components are related by
  $$
  \begin{aligned}
    \langle e_j \vert \psi \rangle &= \sum_k \langle e_j \vert f_k \rangle \langle f_k \vert \psi \rangle \\
    \iff \psi^e_j                  &= \sum_k S_{jk} \psi^f_k \\
    \iff \psi^e                    &= S \psi^f
  \end{aligned}
  $$
  where $S$ is the matrix with elements $S_{jk} = \langle e_j \vert f_k \rangle$.
</p>

<h5>Matrices</h5>

<p>
  A matrix $A$ can be expanded in either basis
  $$
  A = \sum_{j,k} \vert e_j \rangle \langle e_j \vert A \vert e_k \rangle \langle e_k \vert
    = \sum_{j,k} \vert f_j \rangle \langle f_j \vert A \vert f_k \rangle \langle f_k \vert
  $$
  The components are related by
  $$
  \begin{aligned}
    \langle e_j \vert A \vert e_k \rangle &= \sum_{p,q} \langle e_j \vert f_p \rangle \langle f_p \vert A \vert f_q \rangle \langle f_q \vert e_k \rangle \\
    \iff A^e_{jk} &= \sum_{p,q} S_{jp} A^f_{pq} S^{-1}_{qk} \\
    \iff A^e &= S A^f S^{-1}
  \end{aligned}
  $$
  Where
  $$
    \left(SS^{-1}\right)_{jk} =
    \sum_m S_{jm} S^{-1}_{mk} =
    \sum_m \langle e_j \vert f_m \rangle \langle f_m \vert e_k \rangle =
    \langle e_j \vert e_k \rangle =
    \delta_{jk}
  $$
  shows that $S^{-1}$ indeed is the inverse of $S$.
</p>




<h2>Clifford algebra</h2>

TODO
